<?xml version="1.0"?>
<package xmlns="http://schemas.microsoft.com/packaging/2010/07/nuspec.xsd">
  <metadata>
    <id>Stanford.NLP.Segmenter</id>
    <version>0.0.1</version>
    <authors>The Stanford Natural Language Processing Group</authors>
    <owners>Sergey Tihon</owners>
    <licenseUrl>http://www.gnu.org/licenses/gpl-2.0.html</licenseUrl>
    <iconUrl>https://nlp.stanford.edu/static/img/logos/nlp-logo.gif</iconUrl>
    <projectUrl>http://sergey-tihon.github.io/Stanford.NLP.NET/StanfordWordSegmenter.html</projectUrl>
    <summary>Stanford Word Segmenter</summary>
    <description>Tokenization of raw text is a standard pre-processing step for many NLP tasks. For English, tokenization usually involves punctuation splitting and separation of some affixes like possessives. Other languages require more extensive token pre-processing, which is usually called segmentation.</description>
    <tags>nlp stanford segmenter tokenization splitting</tags>
    <dependencies>
      <dependency id="IKVM" version="[8.1.5717]" />
    </dependencies>
  </metadata>
</package>